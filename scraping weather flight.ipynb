{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f309ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from hdfs import InsecureClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f25076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "    try:\n",
    "        url = \"https://www.flightradar24.com/data/airports\"\n",
    "        driver.get(url)\n",
    "\n",
    "        # Find all links to countries\n",
    "        country_links = driver.find_elements(By.CSS_SELECTOR, 'table#tbl-datatable a[href^=\"https://www.flightradar24.com/data/airports/\"]')\n",
    "        country = []\n",
    "        # Extract and print the country names\n",
    "        for country_link in country_links:\n",
    "            country_name = country_link.get_attribute(\"title\")\n",
    "            country.append(country_name.replace(\" \", \"-\").replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "        country_set = set(country)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        return (country_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40366e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_airport_data(country_url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(country_url)\n",
    "\n",
    "        # Find all airport links\n",
    "        IATA = []\n",
    "        airport_names = []\n",
    "        #k = 0\n",
    "        airport_links = driver.find_elements(By.CSS_SELECTOR, 'a[data-iata][data-lat][data-lon]')\n",
    "        for airport_link in airport_links:\n",
    "            airport_name = airport_link.text.strip().split('\\n')[0]\n",
    "            airport_iata = airport_link.get_attribute('data-iata')\n",
    "            airport_icao = airport_name.split('(')[-1].split(')')[0]\n",
    "            IATA.append(airport_iata)\n",
    "            airport_names.append(airport_name)\n",
    "        airport_names_cleaned = [airport.split(' (')[0] for airport in airport_names]\n",
    " \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return IATA , airport_names_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fdfae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather_data (airport_code) :\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "    \n",
    "    airport_url = \"https://www.flightradar24.com/data/airports/\"+airport_code+\"/weather\"\n",
    "    try:\n",
    "        driver.get(airport_url)\n",
    "\n",
    "        # Wait for the button to appear and accept cookies\n",
    "        time.sleep(15)\n",
    "        try:\n",
    "            button1 = driver.find_element(By.ID, 'onetrust-accept-btn-handler')\n",
    "            button1.click()\n",
    "        except NoSuchElementException:\n",
    "            pass  # If the accept cookies button is not found, continue without clicking\n",
    "        \n",
    "        try:\n",
    "            # Find the element with the specified class name\n",
    "            alert_element = driver.find_element(By.CSS_SELECTOR, 'div.alert.alert-info.text-center')\n",
    "        except:\n",
    "            alert_element = \"No alert\" \n",
    "        if alert_element == \"No alert\" :\n",
    "            element = driver.find_elements(By.CSS_SELECTOR,'td[colspan=\"2\"] > ul')\n",
    "            weather_data = []\n",
    "            for e in element:\n",
    "                soup = e.get_attribute('outerHTML')\n",
    "                sp = BeautifulSoup(soup, 'html.parser')\n",
    "                \n",
    "                ul_tag = sp.find('ul')\n",
    "                # Extract all the information\n",
    "                information = {}\n",
    "                for li_tag in ul_tag.find_all('li'):\n",
    "                    key = li_tag.find('strong').text.strip().rstrip(':')\n",
    "                    value = li_tag.text.strip().replace(key + ':', '').strip()\n",
    "                    information[key] = value\n",
    "\n",
    "            \n",
    "                # Append data to the list as a dictionary\n",
    "                weather_data.append(information)\n",
    "            df = pd.DataFrame(weather_data)\n",
    "            # Find all elements with the specified classes\n",
    "            elements = driver.find_elements(By.XPATH , '//tr[@class=\"master expandable\"]')\n",
    "            # Extract and print the date from each element\n",
    "            dates = []\n",
    "            for element in elements:\n",
    "                soup = element.get_attribute('outerHTML')\n",
    "                sp = BeautifulSoup(soup, 'html.parser')\n",
    "                tr_element = sp.find('tr', class_='master expandable')\n",
    "                # Extract the text content of the second <td> element within the <tr> element\n",
    "                date_td = tr_element.find_all('td')[1]  # Second <td> element contains the date\n",
    "                date = date_td.text.strip()\n",
    "                \n",
    "                #print(\"Date:\", date)\n",
    "                dates.append(date)\n",
    "            df[\"date\"] = dates\n",
    "            #print (dates, len(dates))\n",
    "            return (df)\n",
    "        else : \n",
    "          \n",
    "            return \"we don't have any data for this ariport\" \n",
    "\n",
    "\n",
    "    finally:\n",
    "            driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5d584",
   "metadata": {},
   "source": [
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ee00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather_from_country (country_name):\n",
    "    url_country = \"https://www.flightradar24.com/data/airports/\" + country_name\n",
    "    IATA , aeroport_names = scrape_airport_data(url_country)\n",
    "    data_total_country = []\n",
    "    for i, j in  zip (IATA, aeroport_names) : \n",
    "        df_aeroport = scrape_weather_data(i) \n",
    "        if (type(df_aeroport) != str):\n",
    "            print(\"data founded in \",j , \" = \" , len(df_aeroport)  )\n",
    "            df_aeroport[\"Original Aeroport\"] =  j \n",
    "            data_total_country.append(df_aeroport)\n",
    "    if len(data_total_country) >0 :\n",
    "        concatenated_df = pd.concat(data_total_country, ignore_index=True)\n",
    "        if isinstance(concatenated_df, pd.DataFrame) :\n",
    "            return concatenated_df\n",
    "    else :\n",
    "        return \"No data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0874df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_hdfs(data, file_path): \n",
    "    client = InsecureClient('http://localhost:50070')\n",
    "    with client.write(file_path, overwrite=True) as writer:\n",
    "        data.to_csv(writer, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd83f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def save_csv(data, parent_folder, folder_name, file_name):\n",
    "    # Create the folder if it doesn't exist\n",
    "    folder_path = os.path.join(parent_folder, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Define the file path within the new folder\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    send_to_hdfs(data , file_path.replace(os.path.sep, '/'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f019b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def scrape_all_arrival_flight() :\n",
    "    countries = get_country()\n",
    "  \n",
    "    #countries =[\"Egypt\",\"Saudi-Arabia\",\"United Arab Emirates\" ,\"Algeria\",\"Morocco\",\"Tunisia\"]\n",
    "   # countries_done = [\"Italy\",\"Somalia\",\"Serbia\",\"Botswana\",\"Aruba\",\"Nigeria\",\"Antigua-And-Barbuda\",\"Democratic-Republic-Of-The-Congo\",\"Czechia\",\"Zimbabwe\",\"Venezuela\",\"Slovakia\",\"Uzbekistan\",\"Equatorial-Guinea\",\"Virgin-Islands-Us\",\"Greenland\",\"Iraq\",\"Argentina\",\"El-Salvador\",'Sierra-Leone',\"Chile\",\"Marshall-Islands\", \"Belarus\",\"Barbados\",\"Suriname\",\"Saint-Helena\",\"Martinique\",\"Ukraine\",\"Egypt\", \"Russia\",\"Martinique\",\"Moldova\",\"Ethiopia\" , \"Dominican-Republic\", \"Austria\",\"Ukraine\",\"New-Caledonia\", \"Senegal\",\"Poland\" , \"Puerto-Rico\",\"Slovenia\" , \"Micronesia\" , \"Mauritius\",\"Turkmenistan\",\"Zambia\",\"tunisia\" , \"Solomon-Islands\" ,\"Cocos-keeling-Islands\" ]\n",
    "    for country in countries :\n",
    "       # if country not in countries_done:\n",
    "            print(\"start of extract data from :\", country)\n",
    "            data = scrape_weather_from_country(country) \n",
    "            print(\"end of extract data from :\", country)\n",
    "           # countries_done.append(country)\n",
    "            # File path to save the CSV data\n",
    "            \n",
    "            date = datetime.now().date()\n",
    "            # File path to save the CSV data\n",
    "            file_name = country + str(date) +\".csv\"\n",
    "            # Save the DataFrame to a CSV file\n",
    "            parent_folder = \"/user/PFE_data/weather_flights\"\n",
    "            #folder name \n",
    "            folder_name = country\n",
    "\n",
    "            if isinstance(data, pd.DataFrame) :\n",
    "                save_csv(data,parent_folder ,folder_name,file_name )\n",
    "                message = '<p style=\"color:green;\">data saved</p>'\n",
    "                display(HTML(message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaa0000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of extract data from : Malaysia\n",
      "data founded in  Alor Setar Sultan Abdul Halim Airport  =  69\n",
      "data founded in  Bintulu Airport  =  71\n",
      "data founded in  Ipoh Sultan Azlan Shah Airport  =  71\n",
      "data founded in  Johor Bahru Senai International Airport  =  73\n",
      "data founded in  Kerteh Airport  =  70\n",
      "data founded in  Kota Bharu Sultan Ismail Petra Airport  =  71\n",
      "data founded in  Kota Kinabalu International Airport  =  142\n",
      "data founded in  Kuala Lumpur International Airport  =  142\n",
      "data founded in  Kuala Lumpur Subang Airport  =  78\n",
      "data founded in  Kuala Terengganu Sultan Mahmud Airport  =  71\n",
      "data founded in  Kuantan Sultan Haji Ahmad Shah Airport  =  71\n",
      "data founded in  Kuching International Airport  =  142\n",
      "data founded in  Kudat Airport  =  71\n",
      "data founded in  Labuan Airport  =  71\n",
      "data founded in  Langkawi International Airport  =  69\n",
      "data founded in  Malacca International Airport  =  70\n",
      "data founded in  Miri Airport  =  69\n",
      "data founded in  Penang International Airport  =  143\n",
      "data founded in  Sandakan Airport  =  70\n",
      "data founded in  Sibu Airport  =  71\n",
      "data founded in  Tawau Airport  =  66\n",
      "end of extract data from : Malaysia\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color:green;\">data saved</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of extract data from : Papua-New-Guinea\n",
      "data founded in  Lae Nadzab Airport  =  41\n",
      "data founded in  Mount Hagen Airport  =  41\n"
     ]
    }
   ],
   "source": [
    "scrape_all_arrival_flight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d72c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
