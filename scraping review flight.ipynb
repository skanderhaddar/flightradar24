{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef932133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException ,ElementNotVisibleException, ElementNotInteractableException,TimeoutException, ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed8f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e423720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "    try:\n",
    "        url = \"https://www.flightradar24.com/data/airports\"\n",
    "        driver.get(url)\n",
    "        # Find all links to countries\n",
    "        country_links = driver.find_elements(By.CSS_SELECTOR, 'table#tbl-datatable a[href^=\"https://www.flightradar24.com/data/airports/\"]')\n",
    "        country = []\n",
    "        # Extract and print the country names\n",
    "        for country_link in country_links:\n",
    "            country_name = country_link.get_attribute(\"title\")\n",
    "            country.append(country_name.replace(\" \", \"-\").replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "        country_set = set(country)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        return (country_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f573590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_airport_data(country_url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(country_url)\n",
    "\n",
    "        # Find all airport links\n",
    "        IATA = []\n",
    "        airport_names = []\n",
    "        #k = 0\n",
    "        airport_links = driver.find_elements(By.CSS_SELECTOR, 'a[data-iata][data-lat][data-lon]')\n",
    "        for airport_link in airport_links:\n",
    "            airport_name = airport_link.text.strip().split('\\n')[0]\n",
    "            airport_iata = airport_link.get_attribute('data-iata')\n",
    "            airport_icao = airport_name.split('(')[-1].split(')')[0]\n",
    "            IATA.append(airport_iata)\n",
    "            airport_names.append(airport_name)\n",
    "        airport_names_cleaned = [airport.split(' (')[0] for airport in airport_names]\n",
    " \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return IATA , airport_names_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868a3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_countries_airports(main_url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(main_url)\n",
    "\n",
    "        # Find all links to countries\n",
    "        country_links = driver.find_elements(By.CSS_SELECTOR, 'table#tbl-datatable a[href^=\"https://www.flightradar24.com/data/airports/\"]')\n",
    "        #p = 0\n",
    "        AEROPORT_IATA = {}\n",
    "        for country_link in set(country_links):\n",
    "            country_url = country_link.get_attribute(\"href\")\n",
    "            print(country_url)\n",
    "            for elem in scrape_airport_data(country_url)[0] : \n",
    "                AEROPORT_IATA[elem] = country_url[44:]\n",
    "\n",
    "            #p = p+1\n",
    "           # if p>10 :\n",
    "               # break \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return AEROPORT_IATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08fe40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_button( driver , button_text):\n",
    "            k = 0\n",
    "            while (True and k<3):\n",
    "                k = k+1\n",
    "                try:\n",
    "                    # Wait for the button to be present and visible\n",
    "                    button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, f'//button[text()=\"{button_text}\"]')))\n",
    "                    # Click the button\n",
    "                    button.click()\n",
    "                    time.sleep(3)\n",
    "                except (ElementClickInterceptedException, ElementNotInteractableException, TimeoutException) as e:\n",
    "                    break\n",
    "                except (ElementNotVisibleException, TimeoutException):\n",
    "                    # Break the loop if the button is not visible or timeout occurs\n",
    "                    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b53ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_flight_data (airport_code) :\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "    airport_url = \"https://www.flightradar24.com/data/airports/\"+airport_code+\"/reviews\"\n",
    "    df = \"no review data\"  # Initialize df outside the try block\n",
    "    try:\n",
    "        driver.get(airport_url)\n",
    "\n",
    "        # Wait for the button to appear and accept cookies\n",
    "        time.sleep(12)\n",
    "        try:\n",
    "            button1 = driver.find_element(By.ID, 'onetrust-accept-btn-handler')\n",
    "            button1.click()\n",
    "        except NoSuchElementException:\n",
    "            pass  # If the accept cookies button is not found, continue without clicking\n",
    "        \n",
    "        \"\"\"def click_button(ButtonName):\n",
    "            k = 0\n",
    "            while k < 10:\n",
    "                try:\n",
    "                    # Attente explicite pour l'élément\n",
    "                    button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'btn-load-reviews')))\n",
    "                    button.click()\n",
    "                    k += 1\n",
    "                except (ElementClickInterceptedException, ElementNotInteractableException):\n",
    "                    break\n",
    "                except TimeoutException:\n",
    "                    # Si le bouton n'est pas trouvé dans le délai, sortir de la boucle\n",
    "                    break\"\"\"\n",
    "        try : \n",
    "            no_comment = driver.find_element(By.CLASS_NAME , 'default-tag')\n",
    "        except : \n",
    "            no_comment = None\n",
    "\n",
    "        if  no_comment is not None and no_comment.text == \"No comments available for this airport\"  : \n",
    "            return \"no review data\"\n",
    "        else :\n",
    "            click_button(driver , \"Load more reviews\")\n",
    "            page_source = driver.page_source\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "            # Parse the HTML code\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            #print(soup)\n",
    "            elements = soup.find_all('div', class_='col-xs-10')\n",
    "            filtered_elements = []\n",
    "            for element in elements:\n",
    "                stars_div = element.find('div', class_='stars')\n",
    "                content_div = element.find('div', class_='content')\n",
    "                subratings_div = element.find('div', class_='subratings')\n",
    "                if stars_div and content_div and subratings_div:\n",
    "                    # Si tous les éléments requis sont trouvés, ajoutez cet élément à la liste filtrée\n",
    "                    filtered_elements.append(element)\n",
    "                    # Extract the content of each element\n",
    "                review_data = []\n",
    "                for e in filtered_elements :   \n",
    "                    stars_title = e.find(class_='stars')['title']\n",
    "                    content = e.find(class_='content').text.strip()\n",
    "                    subratings = e.find_all(class_='col-md-6')\n",
    "                    dictionnaire = {}\n",
    "                    date_element = e.find('span', class_='date pull-right')\n",
    "                    if date_element:\n",
    "                        date = date_element.get('title')\n",
    "                    dictionnaire[\"date\"]= date\n",
    "                    dictionnaire[\"Stars Title\"] = stars_title\n",
    "                    dictionnaire [\"text review\"]= content\n",
    "                    for subrating in subratings:\n",
    "                        subrating_title = subrating['title']\n",
    "                        label = subrating.find(class_='label').text.strip()\n",
    "                        dictionnaire[label]= subrating_title\n",
    "                    review_data.append(dictionnaire)\n",
    "                df = pd.DataFrame(review_data)\n",
    "        return df\n",
    "                \n",
    "    finally : \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfdff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_flight_data(\"alg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "218b7326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_flight_from_country (country_name):\n",
    "    url_country = \"https://www.flightradar24.com/data/airports/\" + country_name\n",
    "    IATA , aeroport_names = scrape_airport_data(url_country)\n",
    "    data_total_country = []\n",
    "    for i, j in  zip (IATA, aeroport_names) : \n",
    "        df_aeroport = scrape_flight_data(i) \n",
    "        if isinstance(df_aeroport, pd.DataFrame):\n",
    "            print(\"data founded in \",j , \" = \" , len(df_aeroport)  )\n",
    "            df_aeroport[\"Destination Aeroport\"] =  j \n",
    "            data_total_country.append(df_aeroport)\n",
    "    if len(data_total_country) >0 :\n",
    "        concatenated_df = pd.concat(data_total_country, ignore_index=True)\n",
    "        return concatenated_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c16e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_hdfs(data, file_path): \n",
    "    client = InsecureClient('http://localhost:50070')\n",
    "    with client.write(file_path, overwrite=True) as writer:\n",
    "        data.to_csv(writer, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7918111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "def save_csv(data, parent_folder, folder_name, file_name):\n",
    "    # Create the folder if it doesn't exist\n",
    "    folder_path = os.path.join(parent_folder, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Define the file path within the new folder\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    send_to_hdfs(data , file_path.replace(os.path.sep, '/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee4f881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def scrape_all_reviews() :\n",
    "    countries = get_country()\n",
    "   # countries_done = [\"India\",\"Antarctica\",\"Sri-Lanka\",\"Dominica\",\"Belize\",\"Gibraltar\",\"Sweden\",\"North-Macedonia\",\"Mexico\",\n",
    "                   ##   \"Lithuania\",\"Mozambique\",\"Cook-Islands\",\"Democratic-Republic-Of-The-Congo\",\"Reunion\",\"Singapore\",\"Saint-Vincent-And-The-Grenadines\",\"Mauritania\",\"Zambia\",\"Wallis-And-Futuna\" ,\"Cameroon\",\"Belarus\",\"Kiribati\",\"South-Korea\",\"Uganda\",\"Norway\",\"Guatemala\",\"Poland\",\"Martinique\",\"Greece\",\"Bhutan\",\"Indonesia\", \"Jamaica\",\"Gabon\",\"Fiji\",\"Oman\",\"Niger\",'Syria',\"Tunisia\",\n",
    "                   ##   \"Costa-Rica\",\"Ghana\",\"Algeria\",\"Bosnia-And-Herzegovina\"]\n",
    "    for country in countries :\n",
    "       # if country not in countries_done :\n",
    "            print(\"start of extract data from :\", country)\n",
    "            data = scrape_flight_from_country(country) \n",
    "            print(\"End of extract data from :\", country , \"------------------------\")\n",
    "           # countries_done.append(country)\n",
    "            \n",
    "            date = datetime.now().date()\n",
    "            # File path to save the CSV data\n",
    "            file_name = country + str(date) +\".csv\"\n",
    "            # Save the DataFrame to a CSV file\n",
    "            parent_folder = \"/user/PFE_data/reviews_flights\"\n",
    "            #folder name \n",
    "            folder_name = country\n",
    "\n",
    "            if isinstance(data, pd.DataFrame) :\n",
    "                save_csv(data,parent_folder ,folder_name,file_name )\n",
    "                message = '<p style=\"color:green;\">data saved</p>'\n",
    "                display(HTML(message))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18faa169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of extract data from : Canada\n",
      "data founded in  Abbotsford International Airport  =  1\n"
     ]
    }
   ],
   "source": [
    "scrape_all_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490a35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
